# ğŸªğŸ“ˆ Store Sales Time Series Forecasting

## ğŸ” Project Overview

This project focuses on **Store Sales Prediction** using traditional **Machine Learning regression models** and an **Artificial Neural Network (ANN)**. The goal is to forecast sales accurately by leveraging historical sales data, promotions, holidays, and categorical features such as store number and product family.

The notebook demonstrates a **complete end-to-end ML workflow**, including:

* Data loading
* Exploratory Data Analysis (EDA)
* Feature engineering & preprocessing
* Model training & evaluation
* Performance comparison

---

## ğŸ“‚ Dataset Information

* **Source:** Google Drive (Parquet format)
* **Type:** Time series sales data
* **Target Variable:** `sales`
* **Key Features:**

  * Store number
  * Product family
  * Date-based features (day, month, year)
  * Promotions
  * Holidays

---

## ğŸ§° Libraries & Tools Used

### ğŸ Core Python Libraries

* `pandas`, `numpy`
* `matplotlib`, `seaborn`

### ğŸ¤– Machine Learning

* `scikit-learn`

  * Linear Regression
  * KNN Regressor
  * Random Forest Regressor
* `xgboost` (XGBRegressor)

### ğŸ§  Deep Learning

* `Keras (TensorFlow backend)`

---

## ğŸ“Š Exploratory Data Analysis (EDA)

Key analysis performed:

* Dataset shape & structure
* Missing values & duplicates
* Sales distribution
* Store & product family counts
* Correlation analysis
* Removal of zero-sales records

ğŸ“Œ **Outcome:** Cleaned and well-understood dataset ready for modeling.

---

## ğŸ› ï¸ Data Preprocessing & Feature Engineering

âœ” Dropped irrelevant columns (`id`, `date`)
âœ” One-hot encoded categorical features:

* Store numbers
* Product families
* Day, month, year

âœ” Feature scaling:

* Standardized `onpromotion` using `StandardScaler`

---

## âœ‚ï¸ Train-Test Split

* Initial split: **80% Train / 20% Test**
* Secondary split for better generalization

This ensures fair evaluation and reduces overfitting risk.

---

## ğŸ¤– Machine Learning Models Implemented

| Model                | Description               |
| -------------------- | ------------------------- |
| ğŸ“ Linear Regression | Baseline regression model |
| ğŸ“ KNN Regressor     | Distance-based regression |
| ğŸŒ² Random Forest     | Ensemble tree-based model |
| ğŸš€ XGBoost Regressor | Gradient boosting model   |

### ğŸ“ Evaluation Metric

* **Mean Absolute Error (MAE)** used for both training and testing datasets.

---

## ğŸ“ˆ Model Performance Comparison

* Compared **Train MAE vs Test MAE**
* Visualized results using **bar plots**
* Added a **10% error threshold** reference line

ğŸ“Š This helped identify **overfitting vs underfitting** behavior.

---

## ğŸ§  Artificial Neural Network (ANN)

### ğŸ—ï¸ Architecture

* Input layer (113 features)
* Dense layers:

  * 128 neurons (ReLU)
  * 64 neurons (ReLU)
  * 32 neurons (ReLU)
* Output layer:

  * 1 neuron (Linear activation)

### âš™ï¸ Training Details

* Loss: **Mean Absolute Error (MAE)**
* Optimizer: **Adam**
* Epochs: `5`
* Batch size: `256`

### ğŸ“‰ Training Visualization

* Training vs Validation loss curves plotted

---

## ğŸ† Final Results

* MAE calculated for:

  * Machine Learning models
  * ANN model
* Combined comparison using bar charts

ğŸ“Œ **Insight:** Ensemble models and ANN generally outperform basic regression models.

---


## ğŸ“Œ Key Learnings

* Importance of feature engineering in time series data
* Performance comparison across ML & DL models
* Practical use of MAE for regression evaluation
* Handling large datasets efficiently

---


â­ If you find this project helpful, donâ€™t forget to **star the repository**!
